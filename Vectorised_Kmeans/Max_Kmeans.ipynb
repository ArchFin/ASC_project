{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'statsmodels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatches\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpatches\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsa\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstattools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m acf\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'statsmodels'"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import networkx as nx\n",
    "from sklearn.metrics import silhouette_score\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "from statsmodels.tsa.stattools import acf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelocation_TET = 'NDT_all_12thDec_uncleaned.csv'   #file location of the Temporal Experience Traces (TET), note needs to be in csv form. With 'feelings' being in columns and individual sessions able to be identified by a unique Subject, Week and Session combination of columns. See this file for the appropriate set up\n",
    "feelings = ['MetaAwareness','Presence','PhysicalEffort','MentalEffort','Boredom', 'Receptivity', 'EmotionalIntensity', 'Clarity', 'Release', 'Bliss', 'Embodiment', 'Insightfulness', 'Anxiety', 'SpiritualExperience']  #dimensions of the TET to be included in the clustering\n",
    "feelings_diffs = ['MetaAwareness_diff', 'Presence_diff', 'PhysicalEffort_diff',\n",
    "       'MentalEffort_diff', 'Boredom_diff', 'Receptivity_diff',\n",
    "       'EmotionalIntensity_diff', 'Clarity_diff', 'Release_diff', 'Bliss_diff',\n",
    "       'Embodiment_diff', 'Insightfulness_diff', 'Anxiety_diff',\n",
    "       'SpiritualExperience_diff'] #to locate the vectorised version of the feelings columns\n",
    "no_dimensions_PCA = 2 #number of dimensions to run the PCA on\n",
    "no_clust = 3 #number of vectoral clusters, note the total number of clusters will be one greater as the stable cluster is split into 2\n",
    "no_of_jumps = 7 #number of timestep jumps in the TET to form the vectors\n",
    "\n",
    "headers = {'Subject':0, 'Week':1, 'Session':2}\n",
    "colours = {0:'red',1:'green',2:'blue',3:'orange',4:'pink',5:'yellow'}\n",
    "colours_list = ['red','green','blue','orange','pink','yellow']\n",
    "header = 'Week'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd  \n",
    "\n",
    "# Class to handle CSV file operations\n",
    "class csv_splitter:\n",
    "\n",
    "    def __init__(self, excel_file_name):\n",
    "        \"\"\"\n",
    "        Constructor to initialize the CSV file location.\n",
    "        :param excel_file_name: Name of the CSV file to be processed.\n",
    "        \"\"\"\n",
    "        self.excel_file_location = f'c:\\\\Users\\\\hugma\\\\diss\\\\{excel_file_name}'  # Define file path\n",
    "        self.dataframes = None  # Placeholder for storing DataFrame if needed in future methods\n",
    "\n",
    "    def read_excel(self):\n",
    "        \"\"\"\n",
    "        Reads the CSV file and returns it as a pandas DataFrame.\n",
    "        :return: DataFrame containing CSV file data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df_excel_file_name = pd.read_csv(self.excel_file_location)  # Load CSV into DataFrame\n",
    "            return df_excel_file_name  # Return the DataFrame\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading Excel file: {e}\")  # Print error if file reading fails\n",
    "\n",
    "    def split_by_header(self, df_excel_file_name, heading):\n",
    "        \"\"\"\n",
    "        Splits the DataFrame into multiple DataFrames based on unique values in a specified column.\n",
    "        :param df_excel_file_name: The DataFrame to be split.\n",
    "        :param heading: Column name used to split the DataFrame.\n",
    "        :return: Dictionary of DataFrames and an array containing key-value pairs.\n",
    "        \"\"\"\n",
    "        # Check if the specified heading exists in the DataFrame\n",
    "        if heading not in df_excel_file_name.columns:\n",
    "            print(f\"Error: '{heading}' not found in DataFrame columns.\")  # Print error if column is missing\n",
    "            return None  # Return None to indicate failure\n",
    "\n",
    "        # Get unique values from the specified column\n",
    "        heading_values = df_excel_file_name[heading].unique()\n",
    "\n",
    "        # Create a dictionary where keys are unique values, and values are corresponding filtered DataFrames\n",
    "        split_df = {value: df_excel_file_name[df_excel_file_name[heading] == value] for value in heading_values}\n",
    "\n",
    "        # Convert dictionary into an array format with key-value pairs\n",
    "        split_df_array = [[key, value] for key, value in split_df.items()]\n",
    "\n",
    "        return split_df, split_df_array  # Return both dictionary and array of split DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_splitter_instance = csv_splitter(filelocation_TET)\n",
    "df_csv_file_original = csv_splitter_instance.read_excel()\n",
    "split_df, split_csv_array = csv_splitter_instance.split_by_header(df_csv_file_original,header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finds and plots the principal componants for all of the TET data\n",
    "\n",
    "class principal_component_finder:\n",
    "\n",
    "    def __init__(self,csv_file, feelings,no_dimensions):\n",
    "        self.csv_file_TET = csv_file[feelings]\n",
    "        corr_matrix = self.csv_file_TET.corr()\n",
    "        pca = PCA(n_components=no_dimensions)\n",
    "        self.principal_components=pca.fit_transform(corr_matrix)\n",
    "        self.explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    def PCA_TOT(self):\n",
    "        df_TET_feelings_prin = self.csv_file_TET.dot(self.principal_components)\n",
    "        for i in range(0,self.principal_components.shape[1]):\n",
    "            y_values = []\n",
    "            for j in range(0,len(feelings)):\n",
    "                y_values.append(self.principal_components[j][i])\n",
    "            plt.figure()\n",
    "            plt.bar(feelings,y_values)\n",
    "            plt.title(f'principal componant {i+1}')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'C:/Users/hugma/diss/data/principal_componant{i+1}')\n",
    "        plt.figure()\n",
    "        plt.scatter(df_TET_feelings_prin[0],df_TET_feelings_prin[1], s=0.5)\n",
    "        plt.xlabel('principal componant 1 (bored/effort)')\n",
    "        plt.ylabel('principal componant 2 (calm)')\n",
    "        plt.title('Plot of all the data points in PCA space')\n",
    "        plt.xlim(-6,6)\n",
    "        plt.ylim(-1,2)\n",
    "        \n",
    "        labels = [f'Principal Componant {i+1}' for i in range(self.principal_components.shape[1])]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(labels, self.explained_variance_ratio, color='skyblue')\n",
    "\n",
    "        # Adding title and labels\n",
    "        plt.title('Explained Variance Ratio of PCA Components')\n",
    "        plt.xlabel('Principal Components')\n",
    "        plt.ylabel('Explained Variance Ratio')\n",
    "        plt.xticks(rotation=45)  # Rotates labels to prevent overlap\n",
    "        plt.tight_layout()    \n",
    "        \n",
    "        return self.principal_components, self.explained_variance_ratio, df_TET_feelings_prin\n",
    "    def PCA_split(self,split_df_array):\n",
    "        split_df_array_TET = [[split_df_array[i][0],split_df_array[i][1][feelings]] for i in range(0,len(split_df_array))]\n",
    "        split_csv_TET = {split_df_array_TET[i][0]: split_df_array_TET[i][1] for i in range(0,len(split_df_array))}\n",
    "        df_TET_feelings_prin_dict = {name: split_csv_TET[name].dot(self.principal_components) for name in split_csv_TET.keys()}\n",
    "        \n",
    "        for key,value in df_TET_feelings_prin_dict.items():\n",
    "            plt.figure()\n",
    "            plt.scatter(value[0],value[1],s=0.5)\n",
    "            plt.title(key)\n",
    "            plt.xlabel('principal componant 1 (bored/effort)')\n",
    "            plt.ylabel('principal componant 2 (calm)')\n",
    "            plt.xlim(-6,6)\n",
    "            plt.ylim(-1,2)\n",
    "            plt.show()\n",
    "        return df_TET_feelings_prin_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_Means_Vector_Clustering(df_csv_file_original, feelings, feelings_diffs, principal_components):\n",
    "    split_dict_skip = {}\n",
    "    for (subject, week, session), group in df_csv_file_original.groupby(['Subject', 'Week', 'Session']):\n",
    "        # Skip rows based on no_of_jumps\n",
    "        group = group.iloc[::no_of_jumps].copy()\n",
    "        \n",
    "        # Calculate the diff for each feeling within this group\n",
    "        for feeling in feelings:\n",
    "            group[f'{feeling}_diff'] = -group[feeling].diff(-1)\n",
    "        \n",
    "        # Store the modified group in the dictionary\n",
    "        split_dict_skip[(subject, week, session)] = group\n",
    "\n",
    "    # Concatenate all the groups into a single DataFrame\n",
    "    df_csv_file = pd.concat([df for df in split_dict_skip.values()])\n",
    "    split_dict = {}\n",
    "    for (subject, week, session), group in df_csv_file.groupby(['Subject', 'Week', 'Session']):\n",
    "        split_dict[(subject, week, session)] = group.copy()\n",
    "    differences_array = pd.concat([df[:-1] for df in split_dict.values()])\n",
    "    numbers = []\n",
    "    for i in range(differences_array.shape[0]):\n",
    "        numbers.append(i)\n",
    "    \n",
    "    differences_array_MI = differences_array.copy()\n",
    "    differences_array_MI['number'] = numbers\n",
    "    wcss_best = 10000000\n",
    "    labels_fin = []\n",
    "    cluster_centres_fin = []\n",
    "    for i in range(0,1000):\n",
    "        kmeans = KMeans(3)\n",
    "        kmeans.fit(differences_array.iloc[:,-14:])\n",
    "        labels = kmeans.labels_\n",
    "        cluster_centres = kmeans.cluster_centers_\n",
    "        wcss = kmeans.inertia_\n",
    "        if wcss< wcss_best:\n",
    "            wcss_best = wcss\n",
    "            labels_fin = labels\n",
    "            cluster_centres_fin = cluster_centres\n",
    "    differences_array_MI['labels unnormalised vectors'] = labels_fin\n",
    "    point_colours = []\n",
    "    for i in labels_fin:\n",
    "        point_colours.append(colours[i])\n",
    "    differences_array[[\"principal componant 1\", \"principal componant 2\"]] = differences_array.iloc[:,-14:].dot(principal_components)\n",
    "    plt.xlabel(\"principal componant 1 (bored/effort)\")\n",
    "    plt.ylabel(\"principal componant 2 (calm)\")\n",
    "    plt.title(\"scatter plot of k means cluster TET data vectors\")\n",
    "    plt.scatter(differences_array[\"principal componant 1\"],differences_array[\"principal componant 2\"],color=point_colours, s=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"C:/Users/hugma/diss/data/K_means_vector_scatter_plot\")\n",
    "    plt.show()\n",
    "    cluster_centres_prin = np.transpose(cluster_centres_fin.dot(principal_components),(1,0))\n",
    "    for i in range(cluster_centres_prin.shape[1]):\n",
    "        plt.arrow(0, 0, cluster_centres_prin[0,i], cluster_centres_prin[1,i],\n",
    "                head_width=0.1, head_length=0.1, fc=colours_list[i], ec=colours_list[i])\n",
    "\n",
    "    # Set the labels for the axes\n",
    "    plt.xlabel(\"principal component 1 (bored*effort)\")\n",
    "    plt.ylabel(\"principal component 2 (calm)\")\n",
    "\n",
    "    # Create the legend\n",
    "    legend = ['Cluster {}'.format(i+1) for i in range(cluster_centres_prin.shape[1])]\n",
    "    plt.legend(legend)\n",
    "    plt.title(\"Cluster centres for K-Means Vector\")\n",
    "    plt.xlim(-1.2,1.2)\n",
    "    plt.ylim(-1.2,1.2)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"c:/Users/hugma/diss/data/cluster_centres_for_kmeans_vectors\")\n",
    "    for i in range(0,cluster_centres_fin.shape[0]): \n",
    "        plt.figure() \n",
    "        plt.bar(feelings,cluster_centres_fin[i])\n",
    "        plt.ylim(-0.22,0.22)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.title(f\"cluster centroid for cluster {i+1}  \")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'C:/Users/hugma/diss/data/Vector_Cluster_Centroids_{i}')\n",
    "    magnitudes = []\n",
    "    for i in range (len(cluster_centres_fin)):\n",
    "        magnitudes.append(np.linalg.norm(cluster_centres_fin[i]))\n",
    "    stable_cluster = np.argmin(magnitudes)\n",
    "    differences_array['clust'] = labels_fin\n",
    "\n",
    "    df_stable = differences_array[differences_array['clust'] == stable_cluster].copy() \n",
    "    wcss_best = float('inf') \n",
    "    for i in range(1000):\n",
    "        kmeans = KMeans(2)\n",
    "        kmeans.fit(df_stable[feelings])\n",
    "        labels_test = kmeans.labels_\n",
    "        cluster_centres = kmeans.cluster_centers_\n",
    "        wcss = kmeans.inertia_\n",
    "        if wcss < wcss_best:\n",
    "            wcss_best = wcss\n",
    "            labels_fin_stable = labels_test\n",
    "            cluster_centres_fin_stable = cluster_centres\n",
    "    df_stable = df_stable.drop('clust', axis=1)\n",
    "    df_stable['clust_name'] = [f'{stable_cluster+1}a' if label == 0 else f'{stable_cluster+1}b' for label in labels_fin_stable]\n",
    "    df_stable['clust'] = [stable_cluster if label == 0 else 3 for label in labels_fin_stable]\n",
    "    differences_array['clust_name'] = differences_array['clust']+1\n",
    "    differences_array.update(df_stable)\n",
    "\n",
    "    clust_labels = differences_array['clust'].unique()+1\n",
    "    clust_name_labels = differences_array['clust_name'].unique()\n",
    "\n",
    "# Create a dictionary mapping 'clust' labels to 'clust_name' labels\n",
    "    dictionary_clust_labels = {clust: clust_name for clust, clust_name in zip(clust_labels, clust_name_labels)}\n",
    "    alphabet = {0:'a', 1:'b'}\n",
    "    for i in range(0,cluster_centres_fin_stable.shape[0]): \n",
    "        plt.figure() \n",
    "        plt.bar(feelings,cluster_centres_fin_stable[i])\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.ylim(0,1)\n",
    "        plt.title(f\"cluster centroid for cluster {stable_cluster+1}{alphabet[i]} k-means on stable points\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'C:/Users/hugma/diss/data/Cluster_Centroids_for_stable_cluster_{i}')\n",
    "    cluster_centres_prin_stable = np.transpose(cluster_centres_fin_stable.dot(principal_components),(1,0))\n",
    "    plt.figure()\n",
    "\n",
    "    for i in range(cluster_centres_prin_stable.shape[1]):\n",
    "        cluster_label = f'Cluster {stable_cluster + 1}{alphabet[i]}'\n",
    "        plt.scatter(cluster_centres_prin_stable[0, i], cluster_centres_prin_stable[1, i], label=cluster_label)\n",
    "\n",
    "    plt.xlabel(\"Principal Component 1 (Bored*Effort)\")\n",
    "    plt.ylabel(\"Principal Component 2 (Calm)\")\n",
    "    plt.xlim(-2, 2)\n",
    "    plt.ylim(-1, 1)\n",
    "    plt.legend()  # Add the legend to your plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('C:/Users/hugma/diss/data/stable_cluster_centroids')\n",
    "    plt.show()\n",
    "    num_clusters = cluster_centres_fin_stable.shape[0]\n",
    "    num_feelings = len(feelings)\n",
    "    bar_width = 0.8 / num_clusters  # Adjust width for clarity\n",
    "    offset = np.arange(num_feelings)  # Base offset for each group of bars\n",
    "\n",
    "    plt.figure(figsize=(10, 8))  # Consider adjusting the size for clarity\n",
    "\n",
    "    for i in range(num_clusters):\n",
    "        # Calculate the position for each cluster's bars\n",
    "        positions = [pos + (bar_width * i) for pos in offset]\n",
    "        \n",
    "        cluster_label = f'Cluster {stable_cluster+1}{alphabet[i]}'\n",
    "        plt.bar(positions, cluster_centres_fin_stable[i], width=bar_width, label=cluster_label)\n",
    "\n",
    "    plt.xticks(offset + bar_width * num_clusters / 2, feelings, rotation=45, ha='right')  # Adjust tick positions and labels\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(\"Cluster centroids for k-means on stable points\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('C:/Users/hugma/diss/data/Cluster_Centroids_Bar_plot')\n",
    "    plt.show()\n",
    "    differences_array['clust_name'] = differences_array['clust_name'].astype(str)\n",
    "    (unique, counts) = np.unique(differences_array['clust_name'], return_counts=True)\n",
    "    label_counts = dict(zip(unique, counts))\n",
    "\n",
    "    # Convert keys and values to lists for plotting\n",
    "    labels = list(label_counts.keys())\n",
    "    counts = list(label_counts.values())\n",
    "\n",
    "    # Creating the bar plot\n",
    "    plt.figure(figsize=(10, 5))  # Optional: Adjust figure size as needed\n",
    "    plt.bar(labels, counts)\n",
    "\n",
    "    # Setting x-axis labels to reflect discrete labels\n",
    "    plt.xticks(ticks=np.arange(len(labels)), labels=labels, rotation='vertical')  # Rotate labels if there are many or they are long\n",
    "\n",
    "    # Adding labels and title for clarity\n",
    "    plt.xlabel('Cluster Labels')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.title('Distribution of Cluster Labels')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"C:/Users/hugma/diss/data/Cluster_counts\")\n",
    "    return differences_array, dictionary_clust_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_Means_Vector_Visualise(differences_array, df_csv_file_original, dictionary_clust_labels, principal_components):\n",
    "    differences_array[[\"principal componant 1 non-diff\", \"principal componant 2 non-diff\"]] = differences_array[feelings].dot(principal_components)\n",
    "    traj_transitions_dict = {}\n",
    "    traj_transitions_dict_original = {}\n",
    "    color_map = {\n",
    "        0: 'red',\n",
    "        1: 'green',\n",
    "        2: 'blue',\n",
    "        3: 'yellow',\n",
    "    }\n",
    "    for heading,group in df_csv_file_original.groupby(['Subject','Week','Session']):\n",
    "        traj_transitions_dict_original[heading] = group\n",
    "    for heading,group in differences_array.groupby(['Subject','Week','Session']):\n",
    "        traj_transitions_dict[heading] = group\n",
    "\n",
    "    for heading,value in traj_transitions_dict_original.items():\n",
    "        plt.figure()\n",
    "        for feeling in feelings:\n",
    "            starting_time = 0 \n",
    "            time_jump = 28\n",
    "            time_array = np.arange(starting_time, starting_time + time_jump * value.shape[0], time_jump)\n",
    "            plt.plot(time_array,value[feeling]*10,label=feeling)\n",
    "        combined = ''.join(heading)\n",
    "\n",
    "\n",
    "        cleaned = combined.replace(\"\\\\\", \"\").replace(\"'\", \"\").replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        \n",
    "        \n",
    "        plt.title(f'{cleaned}')    \n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Rating')\n",
    "        plt.tight_layout() \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        prev_color_val = traj_transitions_dict[heading]['clust'].iloc[0]\n",
    "        start_index = 0\n",
    "        for index, color_val in enumerate(traj_transitions_dict[heading]['clust']):\n",
    "            if color_val != prev_color_val or index == traj_transitions_dict[heading].shape[0] - 1:\n",
    "                # Only go up to the previous index if it's not the last span\n",
    "                end_index = index*(time_jump*no_of_jumps) if index != traj_transitions_dict[heading].shape[0] - 1 else time_array[-1]\n",
    "                plt.axvspan(start_index*(time_jump*no_of_jumps), end_index, facecolor=color_map[prev_color_val], alpha=0.3)\n",
    "                start_index = index\n",
    "                prev_color_val = color_val\n",
    "        cluster_patches = [mpatches.Patch(color=color, label=f'Cluster {cluster}') for cluster, color in color_map.items()]\n",
    "        \n",
    "        cluster_patches = [mpatches.Patch(color=color, label=f'Cluster {cluster}') for cluster, color in color_map.items()]\n",
    "        \n",
    "        # Combine feelings and cluster legends into one\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        handles.extend(cluster_patches)\n",
    "        labels.extend([f'Cluster {cluster}' for cluster in dictionary_clust_labels.values()])\n",
    "        \n",
    "        plt.legend(handles=handles, labels=labels, title='Legend', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "\n",
    "        plt.savefig(f\"C:/Users/hugma/diss/data/TET_PLOT_final{cleaned}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_no_jumps_stability(df_csv_file_original, feelings):\n",
    "    y_labels=[]\n",
    "    x_labels=[]\n",
    "    for j in range(1,30):\n",
    "        split_dict_skip = {}\n",
    "        for (subject, week, session), group in df_csv_file_original.groupby(['Subject', 'Week', 'Session']):\n",
    "                # Skip rows based on `j`\n",
    "            group = group.iloc[::j].copy()\n",
    "\n",
    "            # Calculate differences for each feeling before storing in the dictionary\n",
    "            for feeling in feelings:\n",
    "                group[f'{feeling}_diff'] = -group[feeling].diff(-1)\n",
    "\n",
    "            split_dict_skip[(subject, week, session)] = group\n",
    "\n",
    "        # Concatenate all groups into a single DataFrame\n",
    "        df_csv_file_new = pd.concat([df for df in split_dict_skip.values()])\n",
    "        split_dict = {}\n",
    "        for (subject, week, session), group in df_csv_file_new.groupby(['Subject', 'Week', 'Session']):\n",
    "            split_dict[(subject, week, session)] = group.copy()\n",
    "            differences_array = pd.concat([df[:-1] for df in split_dict.values()])\n",
    "        numbers = []\n",
    "        for i in range(differences_array.shape[0]):\n",
    "            numbers.append(i)\n",
    "        \n",
    "        differences_array_MI = differences_array.copy()\n",
    "        differences_array_MI['number'] = numbers\n",
    "        wcss_best = 10000000\n",
    "        labels_fin = []\n",
    "        cluster_centres_fin = []\n",
    "        for i in range(0,1000):\n",
    "            kmeans = KMeans(3)\n",
    "            kmeans.fit(differences_array.iloc[:,-14:])\n",
    "            labels = kmeans.labels_\n",
    "            cluster_centres = kmeans.cluster_centers_\n",
    "            wcss = kmeans.inertia_\n",
    "            if wcss< wcss_best:\n",
    "                wcss_best = wcss\n",
    "                labels_fin = labels\n",
    "                cluster_centres_fin = cluster_centres\n",
    "        differences_array_MI['labels unnormalised vectors'] = labels_fin\n",
    "        (unique, counts) = np.unique(labels, return_counts=True)\n",
    "        magnitudes = []\n",
    "        for i in range (len(cluster_centres)):\n",
    "            magnitudes.append(np.linalg.norm(cluster_centres[i]))\n",
    "        \n",
    "        # The function np.unique returns two arrays: one with the unique values and one with their counts\n",
    "        label_counts = dict(zip(unique, counts))\n",
    "        print(f'for {j} time steps our cluster distribution is {label_counts}')\n",
    "        max_clust = [key for key,value in label_counts.items() if value==max(counts)]\n",
    "        if magnitudes[max_clust[0]]==min(magnitudes):\n",
    "            print(\"true\")\n",
    "        else:\n",
    "            print('false')\n",
    "        y_labels.append(max(counts)/(sum(counts)-max(counts)))\n",
    "        x_labels.append(j)\n",
    "    plt.plot(x_labels,y_labels)\n",
    "    plt.title('Stable Cluster Dominance with No. of Time Steps')\n",
    "    plt.xlabel('Number of Time Jumps')\n",
    "    plt.ylabel('No in stable cluster:No in all other clusters')\n",
    "    plt.savefig(\"C:/Users/hugma/diss/data/stable_cluster_dominance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_no_jumps_consistency(df_csv_file_original, feelings, feelings_diffs):\n",
    "    x_values=[]\n",
    "    y_values = []\n",
    "    for no_of_jumps in range(1,30):\n",
    "        split_dict_skip = {}\n",
    "        for (subject, week, session), group in df_csv_file_original.groupby(['Subject', 'Week', 'Session']):\n",
    "                # Skip rows based on `j`\n",
    "            group = group.iloc[::no_of_jumps].copy()\n",
    "\n",
    "            # Calculate differences for each feeling before storing in the dictionary\n",
    "            for feeling in feelings:\n",
    "                group[f'{feeling}_diff'] = -group[feeling].diff(-1)\n",
    "\n",
    "            split_dict_skip[(subject, week, session)] = group\n",
    "\n",
    "        # Concatenate all groups into a single DataFrame\n",
    "        df_csv_file_new = pd.concat([df for df in split_dict_skip.values()])\n",
    "        split_dict = {}\n",
    "        for (subject, week, session), group in df_csv_file_new.groupby(['Subject', 'Week', 'Session']):\n",
    "            split_dict[(subject, week, session)] = group.copy()\n",
    "            differences_array = pd.concat([df[:-1] for df in split_dict.values()])\n",
    "        numbers = []\n",
    "        for i in range(differences_array.shape[0]):\n",
    "            numbers.append(i)\n",
    "\n",
    "        differences_array_MI = differences_array.copy()\n",
    "        differences_array_MI['number'] = numbers\n",
    "        wcss_best = 10000000\n",
    "        labels_fin = []\n",
    "        cluster_centres_fin = []\n",
    "        for i in range(0,1000):\n",
    "            kmeans = KMeans(3)\n",
    "            kmeans.fit(differences_array.iloc[:,-14:])\n",
    "            labels = kmeans.labels_\n",
    "            cluster_centres = kmeans.cluster_centers_\n",
    "            wcss = kmeans.inertia_\n",
    "            if wcss< wcss_best:\n",
    "                wcss_best = wcss\n",
    "                labels_fin = labels\n",
    "                cluster_centres_fin = cluster_centres\n",
    "        differences_array_MI['labels unnormalised vectors'] = labels_fin\n",
    "        downsampled_groups = []\n",
    "        for (subject, week, session), group in df_csv_file_original.groupby(['Subject', 'Week', 'Session']):\n",
    "            downsampled = group.iloc[::no_of_jumps].copy()\n",
    "            downsampled = downsampled[:-1]\n",
    "            downsampled['Original_Index'] = downsampled.index\n",
    "            downsampled_groups.append(downsampled)\n",
    "\n",
    "        # Concatenate downsampled groups into a DataFrame\n",
    "        df_downsampled = pd.concat(downsampled_groups)\n",
    "        df_downsampled['Cluster_Label'] = labels_fin\n",
    "        df_csv_file_original['Cluster_Label'] = np.nan\n",
    "        for _, row in df_downsampled.iterrows():\n",
    "            original_index = row['Original_Index']\n",
    "            label = row['Cluster_Label']\n",
    "            # Find the group of the original index\n",
    "            group_info = df_csv_file_original.loc[original_index, ['Subject', 'Week', 'Session']]\n",
    "            group_mask = (df_csv_file_original['Subject'] == group_info['Subject']) & \\\n",
    "                        (df_csv_file_original['Week'] == group_info['Week']) & \\\n",
    "                        (df_csv_file_original['Session'] == group_info['Session'])\n",
    "            group_indices = df_csv_file_original[group_mask].index\n",
    "            # Find the position of the original index within its group\n",
    "            pos_in_group = list(group_indices).index(original_index)\n",
    "            # Calculate the start and end indices to assign labels within the group based on no_of_jumps\n",
    "            start_idx = pos_in_group - (pos_in_group % no_of_jumps)\n",
    "            end_idx = min(start_idx + no_of_jumps, len(group_indices))\n",
    "            for i in range(start_idx, end_idx):\n",
    "                df_csv_file_original.at[group_indices[i], 'Cluster_Label'] = label\n",
    "        for feeling in feelings: \n",
    "\n",
    "\n",
    "                df_csv_file_original[f'{feeling}_diff'] = -df_csv_file_original[feeling].diff(-1)\n",
    "        n_entries = 0  # This will count entries considered in the calculation\n",
    "        correct_assignments = 0\n",
    "\n",
    "        for i, row in df_csv_file_original.iterrows():\n",
    "            if not pd.isnull(row['Cluster_Label']) and not row[feelings_diffs].isnull().any():\n",
    "                # Both cluster label and feelings_diffs are not NaN\n",
    "                entry = row[feelings_diffs]*no_of_jumps # Extract features\n",
    "                assigned_cluster = row['Cluster_Label']\n",
    "                \n",
    "                # Calculate distances to all cluster centers\n",
    "                distances = np.array([distance.euclidean(entry, centre) for centre in cluster_centres_fin])\n",
    "                \n",
    "                # Find the index of the closest cluster center\n",
    "                closest_centre_idx = np.argmin(distances)\n",
    "                \n",
    "                # Check if the assigned cluster is the closest\n",
    "                if closest_centre_idx == assigned_cluster:\n",
    "                    correct_assignments += 1\n",
    "                \n",
    "                n_entries += 1  # Increment count of entries considered\n",
    "\n",
    "        if n_entries > 0:\n",
    "            hughes_measure = correct_assignments / n_entries\n",
    "        y_values.append(hughes_measure)\n",
    "        x_values.append(no_of_jumps)\n",
    "    plt.plot(x_values,y_values)\n",
    "    plt.title('Cluster Vectoral Consistency')\n",
    "    plt.xlabel('Number of Time Steps')\n",
    "    plt.ylabel('Correct Assignment Ratio')\n",
    "    plt.savefig(\"C:/Users/hugma/diss/data/Cluster_Vectoral_consistency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_no_of_jumps_autocorrelation(df_csv_file_original, feelings):\n",
    "    split_dict = {}\n",
    "    for (subject, week, session), group in df_csv_file_original.groupby(['Subject', 'Week', 'Session']):\n",
    "        split_dict[(subject, week, session)] = group[feelings].copy()\n",
    "    acf_results = {feeling: [] for feeling in feelings}\n",
    "\n",
    "    # Number of lags for ACF calculation\n",
    "    n_lags = 30\n",
    "\n",
    "    # Calculate ACF for each feeling in each DataFrame\n",
    "    for key, df in split_dict.items():\n",
    "        for feeling in feelings:\n",
    "            acf_value = acf(df[feeling], nlags=n_lags, fft=True)\n",
    "            acf_results[feeling].append(acf_value)\n",
    "\n",
    "    # Average the ACFs across all entries for each feeling\n",
    "    acf_averages = {feeling: np.mean(np.vstack(acf_results[feeling]), axis=0) for feeling in feelings}\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for feeling, acf_vals in acf_averages.items():\n",
    "        plt.plot(acf_vals, label=feeling)\n",
    "\n",
    "    plt.title('Average Autocorrelation Function for Each Feeling')\n",
    "    plt.xlabel('Lag')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.legend(title='Feeling', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout() \n",
    "    plt.savefig(\"C:/Users/hugma/diss/data/Autocorrelation for each feeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "determine_no_of_jumps_autocorrelation(df_csv_file_original, feelings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "determine_no_jumps_stability(df_csv_file_original,feelings) #this plots the ratio of the size of the stable cluster compared to all of the rest combined for various time jumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "determine_no_jumps_consistency(df_csv_file_original, feelings, feelings_diffs) #this provides a score describing the agreement of the cluster assigment for each data point when consider with no time jumps and for varying time jumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the functions to perform PCA\n",
    "csv_splitter_instance = csv_splitter(filelocation_TET)\n",
    "df_csv_file_original = csv_splitter_instance.read_excel()\n",
    "split_df, split_csv_array = csv_splitter_instance.split_by_header(df_csv_file_original,header)\n",
    "principal_component_finder_instance = principal_component_finder(df_csv_file_original,feelings,no_dimensions_PCA)\n",
    "principal_components, explained_variance_ratio, df_TET_feelings_prin = principal_component_finder_instance.PCA_TOT()\n",
    "df_TET_feelings_prin_dict = principal_component_finder_instance.PCA_split(split_csv_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences_array, dictionary_clust_labels = K_Means_Vector_Clustering(df_csv_file_original, feelings, feelings_diffs, principal_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_Means_Vector_Visualise(differences_array, df_csv_file_original, dictionary_clust_labels,principal_components)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
